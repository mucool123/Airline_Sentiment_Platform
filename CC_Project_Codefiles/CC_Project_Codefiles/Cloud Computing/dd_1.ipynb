{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d67cc9f5-bfc2-4ba9-89fb-78c8567cc4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing req. Lib.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3250d-68f2-47de-affd-8d0694f6a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our data set\n",
    "data = pd.read_csv('Tweets.csv')\n",
    "data.head()\n",
    "\n",
    "#tweet_created column got the date recorts and showing type is object we have to change it of date time format\n",
    "data['tweet_created'] = pd.to_datetime(data['tweet_created']).dt.date\n",
    "data['tweet_created'] = pd.to_datetime(data['tweet_created'])\n",
    "\n",
    "print(\"Percentage null or na values in df\")\n",
    "((data.isnull() | data.isna()).sum() * 100 / data.index.size).round(2)\n",
    "\n",
    "del data['tweet_coord']\n",
    "del data['airline_sentiment_gold']\n",
    "del data['negativereason_gold']\n",
    "\n",
    "\n",
    "data.drop(data.loc[data['airline_sentiment']=='neutral'].index, inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(data['airline_sentiment'])\n",
    "\n",
    "data['airline_sentiment_encoded'] = le.transform(data['airline_sentiment'])\n",
    "\n",
    "# cleaning the tweet text data to apply classification algorithms on it\n",
    "\n",
    "import re\n",
    "\n",
    "def tweet_to_words(tweet):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", tweet) \n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return \" \".join(meaningful_words)\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "data['clean_tweet'] = data['text'].apply(lambda x: tweet_to_words(x))\n",
    "\n",
    "\n",
    "### Vectorization\n",
    "x = data.clean_tweet\n",
    "y = data.airline_sentiment\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.25)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train)\n",
    "\n",
    "# Use the trained to create a document-term matrix from train and test sets\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)\n",
    "vect_tunned\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName('SentimentAnalysis').getOrCreate()\n",
    "\n",
    "# Convert pandas dataframe to Spark dataframe\n",
    "sdf = spark.createDataFrame(data)\n",
    "\n",
    "# Define stages for the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"clean_tweet\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "labelIndexer = StringIndexer(inputCol=\"airline_sentiment\", outputCol=\"label\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, labelIndexer, lr])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "(trainingData, testData) = sdf.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    # Create a local DataFrame with the text\n",
    "    local_df = spark.createDataFrame([(text,)], [\"clean_tweet\"])\n",
    "\n",
    "    # Transform the local DataFrame using the model to make a prediction\n",
    "    prediction = model.transform(local_df)\n",
    "\n",
    "    # Extract the predicted label index and probability vector\n",
    "    predicted_label_index = prediction.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "    predicted_probability = prediction.select(\"probability\").collect()[0][\"probability\"].toArray()\n",
    "\n",
    "    # Extract the labels from the StringIndexerModel (which is the last but one stage in the pipeline)\n",
    "    labels = model.stages[-2].labels\n",
    "\n",
    "    # Map the predicted label index (float) to the corresponding string label\n",
    "    predicted_label = labels[int(predicted_label_index)]\n",
    "\n",
    "    # Calculate the confidence\n",
    "    confidence = float(predicted_probability[int(predicted_label_index)])\n",
    "\n",
    "    return predicted_label, confidence\n",
    "\n",
    "# Test the prediction function\n",
    "text_to_predict = \"I had an okay flight with comfortable seats and average customer service\"\n",
    "predicted_sentiment, confidence = predict_sentiment(text_to_predict)\n",
    "print(f\"The predicted sentiment for the text '{text_to_predict}' is: {predicted_sentiment} with confidence {confidence:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6343ba-a7b1-4147-959f-653a78135e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fac55-2547-4c82-a527-e017771b243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Task 1. Prepare 15 slides\n",
    "\n",
    "\n",
    "1-3 - Introdution of you and me , Introdution of our problem statement, Why the need to use cloud ?\n",
    "4-5   Dataset Explanation\n",
    "++ 6-11 - Flow of the project (flowchart), modeling part (accuracy between model),front end part (Website page) and explaining fucntionalities\n",
    "12-15 - why the need of cloud based platforms for this, deployment, usabilty , future work, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dd1e5-57ea-4082-b025-ce331ff2ecbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
